{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Predicting User Engagement in Corporate Collaboration Network\n",
    "\n",
    "by Mike Yea\n",
    "DAT7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Background\n",
    "\n",
    "In 2012, an opt-in, web-based (and mobile-enabled) collaboration network was launched at my organization, a 90,000-employee federal agency.  To date, this tool is the only one of its kind that spans across the entire agency.  While initial roll-out and user adoption were impressive (50 percent of users joined in the first 6 months), the growth rate of the network has slowed.  Among chief complaints from users is a high number of unresponded \"messages\" or posts.  Without active collaboration, the network--which is designed to foster the breaking down of organizational silos and link a geographically distributed workforce--will become a shell of its former self. To prevent such an outcome, my colleagues and I are interested in launching user engagement campaign to \"lift\" user engagement.  Rather than innundating users and potential users with mass marketing, we would like to engage users in an informed way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Problem Statement\n",
    "\n",
    "**Can I predict a “lift” in user engagement (i.e., replies to public messages) from message attributes (e.g., length of the message, message posed in the form of a question, presence of attachments and hyperlink, key words, message tone/sentiment, message text, information about message poster)?**    \n",
    "\n",
    "My initial hypotheses are:\n",
    "1. Message content and metadata have intrinsic value in predicting user engagement\n",
    "2. Message poster's role within the organization and activity level within the network are predictors of user engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Data\n",
    "\n",
    "###3.1 Data Import\n",
    "\n",
    "The collaboration network has approximately 3 years of data, and the data can be exported via a web interface.  I imported both message (messages.csv) and user profile (users.csv) data for the last one year.  I used Pandas to extract data, but encountered some encoding errors. By importing and using the \"sys\" library, I was able to parse the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "messages = pd.read_csv('Messages.csv', na_filter=False)\n",
    "messages.shape\n",
    "messages.describe()\n",
    "messages.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.2 Data Pre-Processing\n",
    "\n",
    "Because the purpose of this study is learning about the public interaction of users, 1) I removed private messages and messages posted in private groups; and 2) stored them in a data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages.in_private_group.sum()\n",
    "messages.in_private_conversation.sum()\n",
    "public_msgs = messages[(messages.in_private_group == False) & (messages.in_private_conversation == False)]\n",
    "public_msgs = public_msgs.drop(['in_private_group', 'in_private_conversation'], axis=1)\n",
    "public_msgs = public_msgs.replace('', 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data fields are as follows (filtered to include fields used in the project):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Message**\n",
    "* id: unique identifier for each message\n",
    "* replied_to_id: id of the message to which the subject message is replying; blank if top-level message\n",
    "* thread_id: id of the top-level message \n",
    "* group_id: id of the network group\n",
    "* group_name: name of the group\n",
    "* participants: user id(s) of participants in the thread\n",
    "* in_private_group: whether or not the message is posted to a private group (boolean)\n",
    "* in_private_conversation: whether or not the message is private (boolean) \n",
    "* sender_id: id of the message's author\n",
    "* body: message text\n",
    "* attachments: internal identifier\n",
    "* created_at: DTG when the message was posted\n",
    "\n",
    "**User**\n",
    "* id: unique identifier for each user\n",
    "* job_title: user entered position description\n",
    "* joined_at: when the user joined the network\n",
    "* state: whether the user is active or not (boolean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.3 Response Variable\n",
    "\n",
    "Additionally, messages in messages.csv are not normalized; both top-level (or headline) messages and replies in the thread are stored in the same table.  Since I primarily am interested in how the top-level messages induce user engagement (i.e., reply to the initial message), I created a separate data frame that only contains top-level messages and added to that data frame a series that contained the number of replies to each top-level message: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. Create response variable column in a dataframe\n",
    "'''\n",
    "#1.1 Initialize a dataframe\n",
    "y_data = pd.DataFrame()\n",
    "y_data = y_data.fillna(0)\n",
    "#1.2 Add number of replies to each original message\n",
    "#1.2.1 Populate dataframe with top-level message ID\n",
    "top_msg_id = []\n",
    "for index, row in public_msgs.iterrows():\n",
    "    if row.id == row.thread_id:\n",
    "        top_msg_id.append(row.id)\n",
    "y_data['id'] = top_msg_id\n",
    "#1.2.2 Create a function that returns the number of replies to a given top-level message ID\n",
    "def get_reply_counts(id):    \n",
    "    public_msgs['num_reply'] = public_msgs.replied_to_id.str.contains(str(id)).astype(int)\n",
    "    return public_msgs['num_reply'].sum()    \n",
    "    \n",
    "#1.2.3 Determine the number of replies to each top-level message and store it to the dataframe\n",
    "cnt_replies = []\n",
    "for row in y_data.id:\n",
    "    cnt_replies.append(get_reply_counts(row))\n",
    "y_data['num_replies'] = cnt_replies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested by several active users, a vast majority, 80.02 percent, of messages in the sample data go unanswered (amazingly enough, this number has hovered around 80% throughout the **history of the collaboration network**).  A histogram of the number of replies is depicted as follows:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hist_num.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this obvious class imbalance, the most frequently appearing class (i.e., no reply) was removed randomly (without replacement) from the dataframe to achieve even distribution of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.2.4 Remove random rows of y_data that has num_replies == 0 to achieve class balance\n",
    "import numpy as np\n",
    "rows = np.random.choice(y_data[y_data.num_replies == 0].index.values, 5774, replace=False)\n",
    "y_data = y_data.drop(y_data.index[rows], in_place=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. Feature Analysis and Selection\n",
    "\n",
    "###4.1 Feature Engineering\n",
    "\n",
    "Given the volume of unstructured data, transforming the body of the message into a document term matrix (DTM) appears to be a good idea (user-entered job title also was transformed into a DTM).  Additionally, there are 9 hand-engineered features that I hypothesized could correlate with the response variable:\n",
    "\n",
    "1. message posted in a group (a proxy for collaborating in self-selected group) (binary)\n",
    "2. attachments (binary)\n",
    "3. length of message (continuous)\n",
    "4. hyperlinks included (binary)\n",
    "5. message tone/sentiment (index between -1 and 1)\n",
    "6. message posed as a question (binary)\n",
    "7. number of key words observed over time (\"experience\", \"opportunity\", and \"interest\") that appear to draw user engagement (continuous)\n",
    "8. message poster's tenure in the collaboration network when a message was posted (number of days; continuous)\n",
    "9. @mentions one or more users (binary)\n",
    "    \n",
    "A number of different approaches was used to engineer the above features.  Shown below as an example is the key word feature (#7) using Regular Expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "2. Add features to the dataframe\n",
    "'''\n",
    "\n",
    "#2.7 Key words [\"experience\", \"opportunity\", \"interest\"] use apply(key_word_search)\n",
    "import re\n",
    "def search_key_words(text):\n",
    "    return len(re.findall(r\"(experience|opportunity|interest)\", text))\n",
    "public_msgs['has_key_word'] = public_msgs.body.apply(search_key_words)\n",
    "df = public_msgs[['id','has_key_word']]\n",
    "y_data = pd.merge(y_data,df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.2 Feature Selection\n",
    "\n",
    "By plotting a scatter plot (response vs. each feature) chart and adding a regression line, it would be possible to determine what features appear to be correlated strongly with the response:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"pair_wise_1.png\"> \n",
    "<img src=\"pair_wise_2.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the linear regression model, it seems all the features, individually and collectively, have limited explanatory power, and the linear model does not appear to be a good fit.  The negative slope of the \"message sentiment/tone\" chart was unexpected; I thought this feature and the response would be correlated positively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5. Modeling\n",
    "\n",
    "###5.1 Conversion to a Classification Model\n",
    "\n",
    "I realized early in the process an OLM regression model would not be a good candidate (R2 value of .05 and RMSE 1.5).  Hence, the continuous response variable was encoded to class, making this a classification problem.  The reponse variable is encoded as follows:\n",
    "    \n",
    "    **1**: one or more replies (*approximately 50% of all responses)\n",
    "    **0**: no reply (*approxmiately 50% of all reponses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "4. Logistic Regression using DTM of body of the message as a feature and other features\n",
    "'''\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy as sp\n",
    "#4.1 Convert the response variable to classes\n",
    "#4.1 Binary: 0 reply to post: 0; 1 or more replies to post: 1\n",
    "y_data['num_replies_class'] = np.where(y_data.num_replies>0,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5.2 Logistic Regression and Naive Bayes (NB)\n",
    "There were 8 models considered for evaluation at the outset: 1) logistic regresson/NB model using message body DTM; 2) logistic regression/NB model using job title of the message author DTM; 3) logistic regression/NB model using both DTMs and a sparse matrix representing the 9 hand-engineered features; and 4) logistic/NB model using on the 9 hand-engineered features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4.2 Add the body of the message to the dataframe\n",
    "df = public_msgs[['id','body']]\n",
    "y_data = pd.merge(y_data,df)\n",
    "#4.3 Split the new DataFrame into training and testing sets\n",
    "feature_cols = ['body', 'job_title', 'in_group', 'has_attach','msg_len', 'has_qm', 'has_key_word', 'author_age', 'has_at_mention']\n",
    "X = y_data[feature_cols]\n",
    "y = y_data['num_replies_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "#4.4 Use CountVectorizer with body of the message only\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "train_dtm = vect.fit_transform(X_train[:, 0])\n",
    "test_dtm = vect.transform(X_test[:, 0])\n",
    "#4.4b Use CountVectorizer with job_title of the author only\n",
    "train_dtm_jt = vect.fit_transform(X_train[:, 1])\n",
    "test_dtm_jt = vect.transform(X_test[:, 1])\n",
    "#4.5 Cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sp.sparse.csr_matrix(X_train[:, 2:].astype(float))\n",
    "#4.6 Combine sparse matrices\n",
    "train_dtm_extra = sp.sparse.hstack((train_dtm, train_dtm_jt, extra))\n",
    "train_dtm_jt_extra = sp.sparse.hstack((train_dtm_jt, extra))\n",
    "#4.7 Repeat for testing set\n",
    "extra = sp.sparse.csr_matrix(X_test[:, 2:].astype(float))\n",
    "test_dtm_extra = sp.sparse.hstack((test_dtm, test_dtm_jt, extra))\n",
    "test_dtm_jt_extra = sp.sparse.hstack((test_dtm_jt, extra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4.8 Use logistic regression with hand engineered features\n",
    "logreg = LogisticRegression(C=.1)\n",
    "logreg.fit(X_train[:, 2:], y_train)\n",
    "y_pred_class = logreg.predict(X_test[:, 2:])\n",
    "y_pred_prob = logreg.predict_proba(extra)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.610\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1]) #.647\n",
    "#4.8a Use NB with hand engineered features\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train[:, 2:], y_train)\n",
    "y_pred_class = nb.predict(X_test[:, 2:])\n",
    "y_pred_prob = nb.predict_proba(extra)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.454\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1]) #.472\n",
    "#4.9 Use logistic regression with the body of the message\n",
    "logreg = LogisticRegression(C=.01)\n",
    "logreg.fit(train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(test_dtm)\n",
    "y_pred_prob = logreg.predict_proba(test_dtm)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.673\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1]) #.709\n",
    "#4.9a Use logistic regression with the job title of the author\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(train_dtm_jt, y_train)\n",
    "y_pred_class = logreg.predict(test_dtm_jt)\n",
    "y_pred_prob = logreg.predict_proba(test_dtm_jt)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.570\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1]) #.622\n",
    "#4.9b Use NB with the body of the message\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm, y_train)\n",
    "y_pred_class = nb.predict(test_dtm)\n",
    "y_pred_prob = nb.predict_proba(test_dtm)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.670\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1]) #.692\n",
    "#4.9c Use NB with the job title of the author\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm_jt, y_train)\n",
    "y_pred_class = nb.predict(test_dtm_jt)\n",
    "y_pred_prob = nb.predict_proba(test_dtm_jt)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.574\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1]) #.623\n",
    "#4.10 Use logistic regression with all features\n",
    "logreg = LogisticRegression(C=.01)\n",
    "logreg.fit(train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(test_dtm_extra)\n",
    "y_pred_prob = logreg.predict_proba(test_dtm_extra)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.673\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1]) #.711\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "#4.10a Use logistic regression with job title and hand engineered features\n",
    "logreg = LogisticRegression(C=.1)\n",
    "logreg.fit(train_dtm_jt_extra, y_train)\n",
    "y_pred_class = logreg.predict(test_dtm_jt_extra)\n",
    "y_pred_prob = logreg.predict_proba(test_dtm_jt_extra)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.650\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1]) #.682\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "#4.10b Use NB with all features\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm_extra, y_train)\n",
    "y_pred_class = nb.predict(test_dtm_extra)\n",
    "y_pred_prob = nb.predict_proba(test_dtm_extra)\n",
    "metrics.accuracy_score(y_test, y_pred_class) #.674\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1]) #.696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Model Evaluation\n",
    "\n",
    "The null accuracy is .511.  Two point performance metrics, class prediction accuracy and area-under-the-curve, are the primary evaluation metric: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model_performance_1.png\"> \n",
    "<img src=\"model_performance_2.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first group of models, logistic regresson and NB model using all features, performed the best, outperforming the null model by 16 percentage points (ref: class prediction accuracy).  I, however, chose the logistic regression model that used only the hand-engineered features, because it is relatively more interpretable and supports the objective of finding \"levers\" of user engagement \"lift.\"     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Findings and Conclusions\n",
    "\n",
    "###6.1 Findings/Conclusions\n",
    "\n",
    "* Do not reject the hypothesis that the body of the message and metadata of the message are predictors of user engagement\n",
    "* Do not reject the hypothesis that the message author's position/title is a predictor of user engagement\n",
    "* The model with the hand-engineered features is chosen for further exploration due to its relatively high interpretability\n",
    "  * Once selecting a model for further analysis, the model was evaluated by feeding all 256 combinations of features\n",
    "  * The model with the following features--'has_attach', 'has_qm', 'has_key_word', 'author_age', and 'has_at_mention'--achieved .628 class prediction accuracy and .654 AUC, rather insignificant improvement over the model using all features (.611 and .647, respectively)\n",
    "* Training models on class-balanced data did more to improve performance than did any tuning or feature engineering on data with class imbalance\n",
    "\n",
    "    \n",
    "###6.2 Future Work\n",
    "\n",
    "* Understanding the role of replies to original posts that garner greater response could improve problem definition and solution\n",
    "* Obtaining message poster reputation, which was not available, can potentially improve the predictive power of model alternatives\n",
    "* Only about 4% of 18,000 users are \"engaged\" (i.e., post content, comment on other users' content, click on the like button) at any given reporting period.  Approximately, 30% of all users are considered \"lurkers.\"  Lurker data is not available but is obtainable, provided I get approval from the CIO.  Using both active user and lurker data is critical to measuring the true engagement level of users  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
